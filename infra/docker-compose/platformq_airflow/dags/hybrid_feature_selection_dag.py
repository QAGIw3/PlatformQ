"""
Airflow DAG for Hybrid Quantum-Classical Feature Selection
"""
import json
from airflow.decorators import dag, task
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.utils.dates import days_ago

# Define the paths for the Spark script and the data
# These paths are relative to the Airflow container's perspective
SPARK_SCRIPT_PATH = "/opt/airflow/processing/spark/feature_selection_spark.py"
INPUT_DATA_PATH = "/opt/airflow/data/sample_data.csv" # A sample data file would need to be in this path
QUBO_OUTPUT_PATH = "/opt/airflow/dags/data/qubo_problem.json"

@dag(
    dag_id="hybrid_quantum_feature_selection",
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False,
    tags=["quantum", "spark", "hybrid"],
    doc_md="""
    ### Hybrid Feature Selection DAG

    This DAG orchestrates a hybrid quantum-classical workflow:
    1.  **Spark Job**: Runs a PySpark script to perform classical feature selection
        (ChiSqSelector) and formulates a QUBO problem for quantum optimization.
    2.  **Quantum Solver**: Triggers the quantum-optimization-service with the
        QUBO problem to find the optimal feature subset.
    """
)
def hybrid_quantum_feature_selection_dag():

    spark_feature_selection = SparkSubmitOperator(
        task_id="spark_feature_selection",
        application=SPARK_SCRIPT_PATH,
        conn_id="spark_default", # Assumes an Airflow connection named 'spark_default' is configured
        verbose=True,
        application_args=[
            "--input", INPUT_DATA_PATH,
            "--output", QUBO_OUTPUT_PATH,
            "--label", "target_variable", # This should be adjusted to the actual label column name
            "--num_features", "10"
        ]
    )

    # This task reads the QUBO file generated by Spark and prepares the data for the HTTP request
    @task
    def prepare_quantum_request(qubo_file_path: str):
        with open(qubo_file_path, 'r') as f:
            qubo_problem = json.load(f)
        
        # This is the request body for the quantum-optimization-service
        return {
            "problem_type": qubo_problem["problem_type"],
            "problem_data": qubo_problem["problem_data"],
            "solver_type": "quantum",
            "algorithm_name": "qaoa",
            "algorithm_params": {"reps": 4} # Example parameters
        }

    # Prepare the request data by calling the Python task
    request_body_task = prepare_quantum_request(QUBO_OUTPUT_PATH)
    
    trigger_quantum_solver = SimpleHttpOperator(
        task_id="trigger_quantum_solver",
        http_conn_id="quantum_optimization_service", # Assumes an HTTP connection is set up in Airflow
        endpoint="/api/v1/solve",
        method="POST",
        headers={"Content-Type": "application/json"},
        # The data is dynamically pulled from the upstream task's XCom output
        data=request_body_task.output.apply(json.dumps),
        response_check=lambda response: response.status_code == 202,
        log_response=True
    )

    spark_feature_selection >> request_body_task >> trigger_quantum_solver

# Instantiate the DAG
hybrid_dag = hybrid_quantum_feature_selection_dag() 