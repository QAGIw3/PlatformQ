# SeaTunnel CDC (Change Data Capture) Configuration
# Real-time data synchronization across platform services

env {
  execution.parallelism = 2
  job.mode = "STREAMING"
  checkpoint.interval = 30000
  
  # CDC specific settings
  read_startup_mode = "earliest"
}

# CDC Source: Capture changes from PostgreSQL databases
source {
  CDC-Postgres {
    result_table_name = "postgres_cdc_stream"
    
    # Database connection
    hostname = "postgres"
    port = 5432
    database = "platformq"
    username = "postgres"
    password = "${POSTGRES_PASSWORD}"
    
    # Tables to monitor
    table_list = [
      "public.compute_contracts",
      "public.settlements",
      "public.sla_violations",
      "public.market_orders",
      "public.user_positions"
    ]
    
    # CDC configuration
    slot_name = "seatunnel_cdc_slot"
    publication_name = "seatunnel_publication"
    
    # Debezium properties
    debezium {
      snapshot.mode = "initial"
      decimal.handling.mode = "string"
      time.precision.mode = "adaptive"
    }
  }
}

# Transform CDC events
transform {
  # Route different table changes
  Router {
    source_table_name = "postgres_cdc_stream"
    route_rules = [
      {
        condition = "table_name = 'compute_contracts'"
        result_table_name = "contract_changes"
      },
      {
        condition = "table_name = 'settlements'"
        result_table_name = "settlement_changes"
      },
      {
        condition = "table_name = 'sla_violations'"
        result_table_name = "violation_changes"
      },
      {
        condition = "table_name = 'market_orders'"
        result_table_name = "order_changes"
      },
      {
        condition = "table_name = 'user_positions'"
        result_table_name = "position_changes"
      }
    ]
  }
  
  # Process contract changes
  Sql {
    source_table_name = "contract_changes"
    result_table_name = "contract_events"
    sql = """
      SELECT 
        op as operation,
        CASE 
          WHEN op = 'DELETE' THEN before.contract_id
          ELSE after.contract_id
        END as contract_id,
        after.resource_type,
        after.quantity,
        after.delivery_start,
        after.status,
        ts_ms as event_timestamp,
        'contract_change' as event_type
      FROM contract_changes
    """
  }
  
  # Process settlement changes
  Sql {
    source_table_name = "settlement_changes"
    result_table_name = "settlement_events"
    sql = """
      SELECT 
        op as operation,
        after.settlement_id,
        after.contract_id,
        after.provisioning_status,
        after.penalty_amount,
        after.failover_used,
        ts_ms as event_timestamp,
        'settlement_change' as event_type
      FROM settlement_changes
      WHERE op IN ('INSERT', 'UPDATE')
    """
  }
}

# Sink: Sync to multiple destinations

# 1. Publish to Pulsar for event streaming
sink {
  Pulsar {
    source_table_name = "contract_events"
    
    service_url = "pulsar://pulsar:6650"
    topic = "persistent://platformq/cdc/contract-changes"
    
    # Producer configuration
    producer {
      send_timeout = 30000
      block_if_queue_full = true
    }
    
    # Message routing
    message_routing_mode = "KEY"
    partition_key_fields = ["contract_id"]
  }
}

sink {
  Pulsar {
    source_table_name = "settlement_events"
    
    service_url = "pulsar://pulsar:6650"
    topic = "persistent://platformq/cdc/settlement-changes"
    
    message_routing_mode = "KEY"
    partition_key_fields = ["settlement_id"]
  }
}

# 2. Sync to Ignite for fast lookups
sink {
  Ignite {
    source_table_name = "contract_events"
    
    ignite_config {
      addresses = ["ignite:10800"]
      cache_name = "contract_cache"
    }
    
    primary_keys = ["contract_id"]
    write_mode = "UPSERT"
  }
}

# 3. Sync positions to Cassandra
sink {
  Cassandra {
    source_table_name = "position_changes"
    
    host = ["cassandra:9042"]
    keyspace = "platformq"
    table = "user_positions_history"
    
    # Time-series structure
    primary_keys = ["user_id", "position_id", "ts_ms"]
    partition_keys = ["user_id", "position_id"]
    clustering_keys = ["ts_ms"]
  }
}

# Quality monitoring pipeline
source {
  JDBC {
    result_table_name = "quality_metrics"
    
    driver = "org.postgresql.Driver"
    url = "jdbc:postgresql://postgres:5432/platformq"
    username = "postgres"
    password = "${POSTGRES_PASSWORD}"
    
    # Monitor data quality
    query = """
      SELECT 
        'settlements' as table_name,
        COUNT(*) as total_records,
        COUNT(CASE WHEN provisioning_status IS NULL THEN 1 END) as null_status_count,
        COUNT(CASE WHEN penalty_amount < 0 THEN 1 END) as invalid_penalty_count,
        NOW() as check_time
      FROM settlements
      WHERE created_at > NOW() - INTERVAL '1 hour'
      
      UNION ALL
      
      SELECT 
        'sla_violations' as table_name,
        COUNT(*) as total_records,
        COUNT(CASE WHEN violation_type IS NULL THEN 1 END) as null_type_count,
        COUNT(CASE WHEN actual_value < 0 THEN 1 END) as invalid_value_count,
        NOW() as check_time
      FROM sla_violations
      WHERE created_at > NOW() - INTERVAL '1 hour'
    """
    
    # Poll every 5 minutes
    poll_interval = 300000
  }
}

transform {
  # Add data quality scores
  Sql {
    source_table_name = "quality_metrics"
    result_table_name = "quality_scores"
    sql = """
      SELECT 
        table_name,
        total_records,
        null_status_count + null_type_count as null_count,
        invalid_penalty_count + invalid_value_count as invalid_count,
        CASE 
          WHEN total_records = 0 THEN 100.0
          ELSE (1 - (null_count + invalid_count) / CAST(total_records AS DOUBLE)) * 100
        END as quality_score,
        check_time
      FROM quality_metrics
    """
  }
}

# Write quality scores to monitoring
sink {
  Elasticsearch {
    source_table_name = "quality_scores"
    
    hosts = ["http://elasticsearch:9200"]
    index = "data-quality-metrics"
    
    bulk_size = 100
  }
}

# Real-time transformation templates
source {
  Pulsar {
    result_table_name = "raw_events"
    
    service_url = "pulsar://pulsar:6650"
    topics = ["persistent://platformq/raw/*"]
    
    subscription_name = "seatunnel-transformer"
    
    # Generic schema
    schema {
      fields {
        event_id = string
        event_type = string
        payload = string
        timestamp = bigint
      }
    }
  }
}

transform {
  # Template-based transformation
  JavaScript {
    source_table_name = "raw_events"
    result_table_name = "transformed_events"
    
    # Transformation logic
    script = """
      function transform(row) {
        var payload = JSON.parse(row.payload);
        
        // Apply transformations based on event type
        switch(row.event_type) {
          case 'compute_order':
            return {
              order_id: payload.order_id,
              user_id: payload.user_id,
              resource_type: payload.resource_type.toUpperCase(),
              quantity: parseFloat(payload.quantity),
              price: parseFloat(payload.price),
              total_value: parseFloat(payload.quantity) * parseFloat(payload.price),
              timestamp: row.timestamp
            };
            
          case 'sla_metric':
            return {
              settlement_id: payload.settlement_id,
              metric_type: payload.type,
              value: parseFloat(payload.value),
              threshold: parseFloat(payload.threshold || 0),
              is_violation: payload.value < payload.threshold,
              timestamp: row.timestamp
            };
            
          default:
            return row;
        }
      }
    """
  }
}

sink {
  # Route transformed events
  Router {
    source_table_name = "transformed_events"
    
    route_rules = [
      {
        output_topic = "persistent://platformq/transformed/orders"
        condition = "event_type = 'compute_order'"
      },
      {
        output_topic = "persistent://platformq/transformed/metrics"
        condition = "event_type = 'sla_metric'"
      }
    ]
  }
} 