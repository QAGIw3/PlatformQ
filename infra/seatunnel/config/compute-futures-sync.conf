# SeaTunnel configuration for Compute Futures data integration
# Syncs data between Pulsar, Ignite, Cassandra, and Elasticsearch

env {
  # SeaTunnel job configuration
  execution.parallelism = 4
  job.mode = "STREAMING"
  checkpoint.interval = 60000
  
  # Spark specific configs (if using Spark engine)
  spark.app.name = "ComputeFuturesDataSync"
  spark.executor.instances = 2
  spark.executor.cores = 2
  spark.executor.memory = "2g"
  
  # Flink specific configs (if using Flink engine)
  flink.job.name = "ComputeFuturesDataSync"
  flink.stream.checkpoint.interval = 60000
}

# Source: Read from Pulsar topics
source {
  Pulsar {
    # Common settings
    result_table_name = "pulsar_compute_events"
    
    # Pulsar connection
    service_url = "pulsar://pulsar:6650"
    admin_url = "http://pulsar:8080"
    
    # Topics to consume
    topics = [
      "persistent://platformq/compute/settlement-initiated",
      "persistent://platformq/compute/sla-violations",
      "persistent://platformq/compute/failover-events",
      "persistent://platformq/compute/market-events"
    ]
    
    # Consumer configuration
    subscription_name = "seatunnel-compute-futures"
    subscription_type = "Shared"
    
    # Schema configuration
    schema {
      fields {
        settlement_id = string
        event_type = string
        timestamp = bigint
        resource_type = string
        quantity = string
        provider_id = string
        buyer_id = string
        payload = string
      }
    }
    
    # Deserialization
    format = "json"
    format.fail-on-missing-field = false
  }
}

# Transform: Process and enrich events
transform {
  # Parse JSON payload
  Sql {
    source_table_name = "pulsar_compute_events"
    result_table_name = "parsed_events"
    sql = """
      SELECT 
        settlement_id,
        event_type,
        timestamp,
        resource_type,
        CAST(quantity AS DECIMAL(18,2)) as quantity_decimal,
        provider_id,
        buyer_id,
        GET_JSON_OBJECT(payload, '$.status') as status,
        GET_JSON_OBJECT(payload, '$.violation_type') as violation_type,
        GET_JSON_OBJECT(payload, '$.penalty_amount') as penalty_amount,
        current_timestamp() as processing_time
      FROM pulsar_compute_events
    """
  }
  
  # Add computed fields
  FieldMapper {
    source_table_name = "parsed_events"
    result_table_name = "enriched_events"
    field_mapper = {
      event_hour = "HOUR(FROM_UNIXTIME(timestamp/1000))"
      event_date = "DATE(FROM_UNIXTIME(timestamp/1000))"
      is_violation = "CASE WHEN event_type = 'sla-violation' THEN 1 ELSE 0 END"
      is_failover = "CASE WHEN event_type = 'failover-event' THEN 1 ELSE 0 END"
    }
  }
}

# Sink: Write to multiple destinations

# 1. Write to Ignite for fast access
sink {
  Ignite {
    source_table_name = "enriched_events"
    
    # Ignite connection
    ignite_config {
      addresses = ["ignite:10800"]
      cache_name = "compute_futures_events"
      
      # Cache configuration
      cache_mode = "PARTITIONED"
      backups = 1
      write_synchronization_mode = "PRIMARY_SYNC"
    }
    
    # Primary key
    primary_keys = ["settlement_id", "timestamp"]
    
    # Write mode
    write_mode = "UPSERT"
  }
}

# 2. Write to Cassandra for time-series storage
sink {
  Cassandra {
    source_table_name = "enriched_events"
    
    # Cassandra connection
    host = ["cassandra:9042"]
    keyspace = "platformq"
    table = "compute_futures_events"
    username = "cassandra"
    password = "${CASSANDRA_PASSWORD}"
    
    # Table schema (will auto-create if not exists)
    create_table_if_not_exists = true
    create_table_column_types = {
      settlement_id = "text"
      event_type = "text"
      timestamp = "bigint"
      resource_type = "text"
      quantity_decimal = "decimal"
      provider_id = "text"
      buyer_id = "text"
      status = "text"
      violation_type = "text"
      penalty_amount = "text"
      event_date = "date"
      event_hour = "int"
      is_violation = "int"
      is_failover = "int"
      processing_time = "timestamp"
    }
    
    # Partition and clustering keys
    primary_keys = ["settlement_id", "event_date", "timestamp"]
    partition_keys = ["settlement_id", "event_date"]
    clustering_keys = ["timestamp"]
  }
}

# 3. Write to Elasticsearch for search and analytics
sink {
  Elasticsearch {
    source_table_name = "enriched_events"
    
    # Elasticsearch connection
    hosts = ["http://elasticsearch:9200"]
    index = "compute-futures-${event_date}"
    index_type = "_doc"
    
    # Authentication (if enabled)
    username = "elastic"
    password = "${ELASTIC_PASSWORD}"
    
    # Index settings
    index_settings = {
      "number_of_shards" = 3
      "number_of_replicas" = 1
    }
    
    # Mapping
    schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
    
    # Bulk settings
    bulk_size = 1000
    bulk_interval = 5000
  }
}

# Additional pipeline for SLA metrics aggregation
source {
  Ignite {
    result_table_name = "sla_metrics_raw"
    
    ignite_config {
      addresses = ["ignite:10800"]
      cache_name = "compute_metrics"
    }
    
    # Query to get recent metrics
    query = """
      SELECT 
        settlement_id,
        AVG(uptime_percent) as avg_uptime,
        AVG(latency_ms) as avg_latency,
        AVG(performance_score) as avg_performance,
        COUNT(CASE WHEN uptime_percent < 99.9 THEN 1 END) as uptime_violations,
        MAX(timestamp) as last_update
      FROM compute_metrics
      WHERE timestamp > CURRENT_TIMESTAMP - INTERVAL '1' HOUR
      GROUP BY settlement_id
    """
    
    # Poll interval
    poll_interval = 30000
  }
}

transform {
  # Calculate SLA compliance score
  Sql {
    source_table_name = "sla_metrics_raw"
    result_table_name = "sla_compliance"
    sql = """
      SELECT 
        settlement_id,
        avg_uptime,
        avg_latency,
        avg_performance,
        uptime_violations,
        CASE 
          WHEN avg_uptime >= 99.9 AND avg_latency <= 50 AND avg_performance >= 0.95
          THEN 'COMPLIANT'
          ELSE 'VIOLATION'
        END as compliance_status,
        last_update
      FROM sla_metrics_raw
    """
  }
}

# Write SLA compliance to Elasticsearch dashboard
sink {
  Elasticsearch {
    source_table_name = "sla_compliance"
    
    hosts = ["http://elasticsearch:9200"]
    index = "compute-sla-compliance"
    
    # Update existing documents
    primary_keys = ["settlement_id"]
    
    # Real-time update
    bulk_size = 100
    bulk_interval = 1000
  }
} 